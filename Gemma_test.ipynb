{
 "cells": [
  {
   "cell_type": "code",
   "id": "4dc7a0b5-70ca-4482-b2b2-920ed6772851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:02:51.702886Z",
     "start_time": "2025-09-20T15:02:50.798699Z"
    }
   },
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login('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')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "871d70cf-21e5-4ea2-8153-d2c66bd22071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:02:56.596977Z",
     "start_time": "2025-09-20T15:02:56.594034Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"DISABLE_TORCH_COMPILE\"] = \"1\"  # HF/Transformers 쪽에서 torch.compile 경로 비활성"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a0b3c1ff-f2e6-436e-a1b1-1e4de15dce4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:02:58.795675Z",
     "start_time": "2025-09-20T15:02:57.849551Z"
    }
   },
   "source": [
    "import torch._dynamo as dynamo\n",
    "dynamo.disable()  # 이후 호출은 Dynamo 없이 수행\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/w5/wzl4gmf1239g7mj54szj1v840000gn/T/ipykernel_4962/3387011133.py\", line 1, in <module>\n",
      "    import torch._dynamo as dynamo\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._dynamo.eval_frame.DisableContext at 0x1075cbf40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "666e8a08-ab57-4a5a-a861-4ee066ff38e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:03:04.332643Z",
     "start_time": "2025-09-20T15:03:04.190614Z"
    }
   },
   "source": [
    "model.config._attn_implementation = \"eager\"   # 또는 \"sdpa\""
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meager\u001B[39m\u001B[38;5;124m\"\u001B[39m   \u001B[38;5;66;03m# 또는 \"sdpa\"\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "da87332d-fc85-413c-8c97-d3060a11fa84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:03:08.278959Z",
     "start_time": "2025-09-20T15:03:07.871400Z"
    }
   },
   "source": [
    "from transformers import AutoProcessor, Gemma3nForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests, torch\n",
    "from io import BytesIO\n",
    "\n",
    "model_id = \"google/gemma-3n-e2b-it\"\n",
    "token = True  # 또는 \"hf_xxx\"\n",
    "\n",
    "# 1) 모델/프로세서 로드 (GPU 자동 배치 권장)\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "model = Gemma3nForConditionalGeneration.from_pretrained(\n",
    "    model_id, token=token, torch_dtype=dtype, device_map=\"auto\"\n",
    ").eval()\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=token)\n",
    "\n",
    "\n",
    "# 3) 메시지 (이미지는 자리표시자)\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":\"You are a helpful assistant.\"}]},\n",
    "    {\"role\":\"user\",\"content\":[\n",
    "        {\"type\":\"text\",\"text\":\"안녕? 반가워\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "# 4) 프롬프트 문자열 생성\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "# 5) 텍스트+이미지 동시 전처리\n",
    "inputs = processor(text=prompt,  return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 6) 디바이스 이동: dtype 캐스팅은 '부동소수 텐서'만!\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        if v.dtype.is_floating_point:                # pixel_values 등만\n",
    "            inputs[k] = v.to(device=device, dtype=dtype)\n",
    "        else:                                        # input_ids, attention_mask 등은 정수 유지\n",
    "            inputs[k] = v.to(device=device)\n",
    "\n",
    "# pad_token_id 안전장치\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Gemma3nForConditionalGeneration' from 'transformers' (/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoProcessor, Gemma3nForConditionalGeneration\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mPIL\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mrequests\u001B[39;00m\u001B[38;5;241m,\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'Gemma3nForConditionalGeneration' from 'transformers' (/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b3810d6-c5e1-4cc2-a652-67d1792e1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f013990-69bd-46ef-a7f1-78fa8e7d6771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb74eea2de245a8b232cf8ee93a3810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "안녕? 반가워\n",
      "model\n",
      "안녕하세요! 만나서 반가워요! 😊 무엇을 도와드릴까요? 궁금한 점이나 필요한 정보가 있다면 언제든지 말씀해주세요. 😊\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ★ 0) 가장 위, 어떤 import보다 먼저!\n",
    "import os\n",
    "os.environ[\"DISABLE_TORCH_COMPILE\"] = \"1\"   # torch.compile 경로 비활성화\n",
    "\n",
    "from transformers import AutoProcessor, Gemma3nForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests, torch\n",
    "from io import BytesIO\n",
    "\n",
    "model_id = \"google/gemma-3n-e2b-it\"\n",
    "dtype = torch.float16  # V100이면 fp16 권장 (bf16 미지원)\n",
    "\n",
    "model = Gemma3nForConditionalGeneration.from_pretrained(\n",
    "    model_id, token=True, torch_dtype=dtype, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "\n",
    "# ★ 1) 어텐션 구현을 Eager로 강제 (Dynamo 민감도↓)\n",
    "model.config._attn_implementation = \"eager\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":\"You are a helpful assistant.\"}]},\n",
    "    {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"안녕? 반가워\"}]}\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 디바이스 이동 (정수는 정수 유지)\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        inputs[k] = v.to(device=device, dtype=dtype) if v.dtype.is_floating_point else v.to(device=device)\n",
    "\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd36739-8452-4304-875f-09ce8d467a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_template = [\n",
    "    {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":\"You are a helpful assistant.\"}]},\n",
    "    {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"{input}\"}]}\n",
    "]\n",
    "\n",
    "messages = message_template\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 디바이스 이동 (정수는 정수 유지)\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        inputs[k] = v.to(device=device, dtype=dtype) if v.dtype.is_floating_point else v.to(device=device)\n",
    "\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf85c4bc-f835-4bb6-a89b-e205e0248d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '너는 어떤 AI인지 설명해줘'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a1a15f-3f15-407e-bdb3-76ca17570c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "너는 어떤 AI인지 설명해줘\n",
      "model\n",
      "저는 Google DeepMind에서 개발한 Gemma라는 대규모 언어 모델입니다. 저는 오픈 웨이트 모델이며, 누구나 자유롭게 사용할 수 있도록 공개되었습니다. \n",
      "\n",
      "저는 텍스트와 이미지를 입력으로 받아 텍스트만 출력합니다. 저는 텍스트를 생성하고, 질문에 답변하고, 다양한 종류의 창의적인 텍스트 형식을 만들 수 있습니다. \n",
      "\n",
      "저는 아직 개발 중이며, 지속적으로 개선되고 있습니다.\n",
      "소요시간 128.267s\n"
     ]
    }
   ],
   "source": [
    "import time, copy\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "message_template = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": \"{input}\"}]},\n",
    "]\n",
    "\n",
    "user_input = '너는 어떤 AI인지 설명해줘'  # 예시\n",
    "\n",
    "# ✅ 템플릿을 복사해서 안전하게 채우기\n",
    "messages = copy.deepcopy(message_template)\n",
    "messages[1][\"content\"][0][\"text\"] = user_input\n",
    "\n",
    "# ✅ 리스트 형태(conversation)를 그대로 전달\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,   # 다음 턴이 assistant가 되도록\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 디바이스/정밀도 이동 (정수는 정수 유지)\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        inputs[k] = v.to(device=device, dtype=dtype) if v.dtype.is_floating_point else v.to(device=device)\n",
    "\n",
    "# pad_token_id 안전장치\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "print(f\"소요시간 {time.time() - start:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "799f660e-f52a-475b-be01-08a57074da50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "너가 할 수 있는 것에 대해 자세히 설명해줘\n",
      "model\n",
      "안녕하세요! 저는 Google에서 개발한 대규모 언어 모델인 Gemma입니다. 저는 텍스트와 이미지를 입력으로 받아 텍스트만 출력하도록 훈련되었습니다. \n",
      "\n",
      "제가 할 수 있는 일은 다음과 같습니다:\n",
      "\n",
      "**텍스트 기반 작업:**\n",
      "\n",
      "*   **텍스트 생성:** 다양한 스타일과 형식으로 텍스트를 생성할 수 있습니다. 예를 들어, 시, 코드, 스크립트, 음악 작품, 이메일, 편지 등 다양한 종류의 텍스트를 만들 수 있습니다.\n",
      "*   **질문 답변:** 질문에 답변\n",
      "소요시간 76.010s\n"
     ]
    }
   ],
   "source": [
    "import time, copy\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "message_template = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": \"{input}\"}]},\n",
    "]\n",
    "\n",
    "user_input = '너가 할 수 있는 것에 대해 자세히 설명해줘'  # 예시\n",
    "\n",
    "# ✅ 템플릿을 복사해서 안전하게 채우기\n",
    "messages = copy.deepcopy(message_template)\n",
    "messages[1][\"content\"][0][\"text\"] = user_input\n",
    "\n",
    "# ✅ 리스트 형태(conversation)를 그대로 전달\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,   # 다음 턴이 assistant가 되도록\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 디바이스/정밀도 이동 (정수는 정수 유지)\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        inputs[k] = v.to(device=device, dtype=dtype) if v.dtype.is_floating_point else v.to(device=device)\n",
    "\n",
    "# pad_token_id 안전장치\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "print(f\"소요시간 {time.time() - start:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2741774a-1b05-4b02-bbc6-5f48d99ac3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "이미지와 텍스트를 동시에 입력받아서 수행할 수 있어?\n",
      "model\n",
      "네, 저는 이미지를 포함한 텍스트를 동시에 입력받아 처리할 수 있습니다. \n",
      "\n",
      "제가 할 수 있는 작업은 다음과 같습니다:\n",
      "\n",
      "* **이미지 분석:** 이미지 내의 객체 인식, 이미지 캡셔닝, 이미지 분류 등 다양한 이미지 분석 작업을 수행할 수 있습니다.\n",
      "* **텍스트 분석:** 텍스트 번역, 요약, 질문 답변, 감성 분석, 텍스트 생성 등 다양한 텍스트 분석 작업을 수행할 수 있습니다.\n",
      "* **이미지-텍스트 연동:** 이미지와 텍스트를 함께 이해하고, 이미지에 대한 설명을 생성하거나, 텍스트에 대한 이미지 검색 등을 수행할 수 있습니다.\n",
      "\n",
      "**어떻게 사용하시고 싶으신가요?**\n",
      "\n",
      "예를 들어, 다음과 같은 질문을 하실 수 있습니다:\n",
      "\n",
      "* \"이 사진에 있는 사람들의 감정을 분석해줘.\" (이미지와 함께 사진을 업로드)\n",
      "* \"이 텍스트를 요약해줘.\" (텍스트를 입력)\n",
      "* \"이 이미지에 대해 설명해줘.\" (이미지를 업로드)\n",
      "* \"이 텍스트에서 특정 단어를 찾아줘.\" (텍스트를 입력)\n",
      "\n",
      "어떤 작업을\n",
      "소요시간 79.304s\n"
     ]
    }
   ],
   "source": [
    "import time, copy\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "message_template = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": \"{input}\"}]},\n",
    "]\n",
    "\n",
    "user_input = '이미지와 텍스트를 동시에 입력받아서 수행할 수 있어?'  # 예시\n",
    "\n",
    "# ✅ 템플릿을 복사해서 안전하게 채우기\n",
    "messages = copy.deepcopy(message_template)\n",
    "messages[1][\"content\"][0][\"text\"] = user_input\n",
    "\n",
    "# ✅ 리스트 형태(conversation)를 그대로 전달\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,   # 다음 턴이 assistant가 되도록\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 디바이스/정밀도 이동 (정수는 정수 유지)\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        inputs[k] = v.to(device=device, dtype=dtype) if v.dtype.is_floating_point else v.to(device=device)\n",
    "\n",
    "# pad_token_id 안전장치\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "print(f\"소요시간 {time.time() - start:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a296d49c-835f-4456-b584-1405f8032ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
