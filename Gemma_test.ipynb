{
 "cells": [
  {
   "cell_type": "code",
   "id": "4dc7a0b5-70ca-4482-b2b2-920ed6772851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:02:51.702886Z",
     "start_time": "2025-09-20T15:02:50.798699Z"
    }
   },
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login('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')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "871d70cf-21e5-4ea2-8153-d2c66bd22071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:02:56.596977Z",
     "start_time": "2025-09-20T15:02:56.594034Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"DISABLE_TORCH_COMPILE\"] = \"1\"  # HF/Transformers ìª½ì—ì„œ torch.compile ê²½ë¡œ ë¹„í™œì„±"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a0b3c1ff-f2e6-436e-a1b1-1e4de15dce4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:02:58.795675Z",
     "start_time": "2025-09-20T15:02:57.849551Z"
    }
   },
   "source": [
    "import torch._dynamo as dynamo\n",
    "dynamo.disable()  # ì´í›„ í˜¸ì¶œì€ Dynamo ì—†ì´ ìˆ˜í–‰\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/w5/wzl4gmf1239g7mj54szj1v840000gn/T/ipykernel_4962/3387011133.py\", line 1, in <module>\n",
      "    import torch._dynamo as dynamo\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._dynamo.eval_frame.DisableContext at 0x1075cbf40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "666e8a08-ab57-4a5a-a861-4ee066ff38e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:03:04.332643Z",
     "start_time": "2025-09-20T15:03:04.190614Z"
    }
   },
   "source": [
    "model.config._attn_implementation = \"eager\"   # ë˜ëŠ” \"sdpa\""
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meager\u001B[39m\u001B[38;5;124m\"\u001B[39m   \u001B[38;5;66;03m# ë˜ëŠ” \"sdpa\"\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "da87332d-fc85-413c-8c97-d3060a11fa84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:03:08.278959Z",
     "start_time": "2025-09-20T15:03:07.871400Z"
    }
   },
   "source": [
    "from transformers import AutoProcessor, Gemma3nForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests, torch\n",
    "from io import BytesIO\n",
    "\n",
    "model_id = \"google/gemma-3n-e2b-it\"\n",
    "token = True  # ë˜ëŠ” \"hf_xxx\"\n",
    "\n",
    "# 1) ëª¨ë¸/í”„ë¡œì„¸ì„œ ë¡œë“œ (GPU ìžë™ ë°°ì¹˜ ê¶Œìž¥)\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "model = Gemma3nForConditionalGeneration.from_pretrained(\n",
    "    model_id, token=token, torch_dtype=dtype, device_map=\"auto\"\n",
    ").eval()\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=token)\n",
    "\n",
    "\n",
    "# 3) ë©”ì‹œì§€ (ì´ë¯¸ì§€ëŠ” ìžë¦¬í‘œì‹œìž)\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":\"You are a helpful assistant.\"}]},\n",
    "    {\"role\":\"user\",\"content\":[\n",
    "        {\"type\":\"text\",\"text\":\"ì•ˆë…•? ë°˜ê°€ì›Œ\"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "# 4) í”„ë¡¬í”„íŠ¸ ë¬¸ìžì—´ ìƒì„±\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "# 5) í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ë™ì‹œ ì „ì²˜ë¦¬\n",
    "inputs = processor(text=prompt,  return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# 6) ë””ë°”ì´ìŠ¤ ì´ë™: dtype ìºìŠ¤íŒ…ì€ 'ë¶€ë™ì†Œìˆ˜ í…ì„œ'ë§Œ!\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        if v.dtype.is_floating_point:                # pixel_values ë“±ë§Œ\n",
    "            inputs[k] = v.to(device=device, dtype=dtype)\n",
    "        else:                                        # input_ids, attention_mask ë“±ì€ ì •ìˆ˜ ìœ ì§€\n",
    "            inputs[k] = v.to(device=device)\n",
    "\n",
    "# pad_token_id ì•ˆì „ìž¥ì¹˜\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Gemma3nForConditionalGeneration' from 'transformers' (/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoProcessor, Gemma3nForConditionalGeneration\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mPIL\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mrequests\u001B[39;00m\u001B[38;5;241m,\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'Gemma3nForConditionalGeneration' from 'transformers' (/opt/miniconda3/envs/project_GGA1/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b3810d6-c5e1-4cc2-a652-67d1792e1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f013990-69bd-46ef-a7f1-78fa8e7d6771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb74eea2de245a8b232cf8ee93a3810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "ì•ˆë…•? ë°˜ê°€ì›Œ\n",
      "model\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”! ðŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ê¶ê¸ˆí•œ ì ì´ë‚˜ í•„ìš”í•œ ì •ë³´ê°€ ìžˆë‹¤ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”. ðŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# â˜… 0) ê°€ìž¥ ìœ„, ì–´ë–¤ importë³´ë‹¤ ë¨¼ì €!\n",
    "import os\n",
    "os.environ[\"DISABLE_TORCH_COMPILE\"] = \"1\"   # torch.compile ê²½ë¡œ ë¹„í™œì„±í™”\n",
    "\n",
    "from transformers import AutoProcessor, Gemma3nForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests, torch\n",
    "from io import BytesIO\n",
    "\n",
    "model_id = \"google/gemma-3n-e2b-it\"\n",
    "dtype = torch.float16  # V100ì´ë©´ fp16 ê¶Œìž¥ (bf16 ë¯¸ì§€ì›)\n",
    "\n",
    "model = Gemma3nForConditionalGeneration.from_pretrained(\n",
    "    model_id, token=True, torch_dtype=dtype, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "\n",
    "# â˜… 1) ì–´í…ì…˜ êµ¬í˜„ì„ Eagerë¡œ ê°•ì œ (Dynamo ë¯¼ê°ë„â†“)\n",
    "model.config._attn_implementation = \"eager\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":\"You are a helpful assistant.\"}]},\n",
    "    {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"ì•ˆë…•? ë°˜ê°€ì›Œ\"}]}\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì´ë™ (ì •ìˆ˜ëŠ” ì •ìˆ˜ ìœ ì§€)\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        inputs[k] = v.to(device=device, dtype=dtype) if v.dtype.is_floating_point else v.to(device=device)\n",
    "\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd36739-8452-4304-875f-09ce8d467a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_template = [\n",
    "    {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":\"You are a helpful assistant.\"}]},\n",
    "    {\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"{input}\"}]}\n",
    "]\n",
    "\n",
    "messages = message_template\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì´ë™ (ì •ìˆ˜ëŠ” ì •ìˆ˜ ìœ ì§€)\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        inputs[k] = v.to(device=device, dtype=dtype) if v.dtype.is_floating_point else v.to(device=device)\n",
    "\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf85c4bc-f835-4bb6-a89b-e205e0248d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'ë„ˆëŠ” ì–´ë–¤ AIì¸ì§€ ì„¤ëª…í•´ì¤˜'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a1a15f-3f15-407e-bdb3-76ca17570c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "ë„ˆëŠ” ì–´ë–¤ AIì¸ì§€ ì„¤ëª…í•´ì¤˜\n",
      "model\n",
      "ì €ëŠ” Google DeepMindì—ì„œ ê°œë°œí•œ Gemmaë¼ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ìž…ë‹ˆë‹¤. ì €ëŠ” ì˜¤í”ˆ ì›¨ì´íŠ¸ ëª¨ë¸ì´ë©°, ëˆ„êµ¬ë‚˜ ìžìœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìžˆë„ë¡ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "ì €ëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ì•„ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. ì €ëŠ” í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ì§ˆë¬¸ì— ë‹µë³€í•˜ê³ , ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì°½ì˜ì ì¸ í…ìŠ¤íŠ¸ í˜•ì‹ì„ ë§Œë“¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "ì €ëŠ” ì•„ì§ ê°œë°œ ì¤‘ì´ë©°, ì§€ì†ì ìœ¼ë¡œ ê°œì„ ë˜ê³  ìžˆìŠµë‹ˆë‹¤.\n",
      "ì†Œìš”ì‹œê°„ 128.267s\n"
     ]
    }
   ],
   "source": [
    "import time, copy\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "message_template = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": \"{input}\"}]},\n",
    "]\n",
    "\n",
    "user_input = 'ë„ˆëŠ” ì–´ë–¤ AIì¸ì§€ ì„¤ëª…í•´ì¤˜'  # ì˜ˆì‹œ\n",
    "\n",
    "# âœ… í…œí”Œë¦¿ì„ ë³µì‚¬í•´ì„œ ì•ˆì „í•˜ê²Œ ì±„ìš°ê¸°\n",
    "messages = copy.deepcopy(message_template)\n",
    "messages[1][\"content\"][0][\"text\"] = user_input\n",
    "\n",
    "# âœ… ë¦¬ìŠ¤íŠ¸ í˜•íƒœ(conversation)ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,   # ë‹¤ìŒ í„´ì´ assistantê°€ ë˜ë„ë¡\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤/ì •ë°€ë„ ì´ë™ (ì •ìˆ˜ëŠ” ì •ìˆ˜ ìœ ì§€)\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        inputs[k] = v.to(device=device, dtype=dtype) if v.dtype.is_floating_point else v.to(device=device)\n",
    "\n",
    "# pad_token_id ì•ˆì „ìž¥ì¹˜\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "print(f\"ì†Œìš”ì‹œê°„ {time.time() - start:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "799f660e-f52a-475b-be01-08a57074da50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "ë„ˆê°€ í•  ìˆ˜ ìžˆëŠ” ê²ƒì— ëŒ€í•´ ìžì„¸ížˆ ì„¤ëª…í•´ì¤˜\n",
      "model\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Googleì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì¸ Gemmaìž…ë‹ˆë‹¤. ì €ëŠ” í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ì•„ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "ì œê°€ í•  ìˆ˜ ìžˆëŠ” ì¼ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "**í…ìŠ¤íŠ¸ ê¸°ë°˜ ìž‘ì—…:**\n",
      "\n",
      "*   **í…ìŠ¤íŠ¸ ìƒì„±:** ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ê³¼ í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‹œ, ì½”ë“œ, ìŠ¤í¬ë¦½íŠ¸, ìŒì•… ìž‘í’ˆ, ì´ë©”ì¼, íŽ¸ì§€ ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
      "*   **ì§ˆë¬¸ ë‹µë³€:** ì§ˆë¬¸ì— ë‹µë³€\n",
      "ì†Œìš”ì‹œê°„ 76.010s\n"
     ]
    }
   ],
   "source": [
    "import time, copy\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "message_template = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": \"{input}\"}]},\n",
    "]\n",
    "\n",
    "user_input = 'ë„ˆê°€ í•  ìˆ˜ ìžˆëŠ” ê²ƒì— ëŒ€í•´ ìžì„¸ížˆ ì„¤ëª…í•´ì¤˜'  # ì˜ˆì‹œ\n",
    "\n",
    "# âœ… í…œí”Œë¦¿ì„ ë³µì‚¬í•´ì„œ ì•ˆì „í•˜ê²Œ ì±„ìš°ê¸°\n",
    "messages = copy.deepcopy(message_template)\n",
    "messages[1][\"content\"][0][\"text\"] = user_input\n",
    "\n",
    "# âœ… ë¦¬ìŠ¤íŠ¸ í˜•íƒœ(conversation)ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,   # ë‹¤ìŒ í„´ì´ assistantê°€ ë˜ë„ë¡\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤/ì •ë°€ë„ ì´ë™ (ì •ìˆ˜ëŠ” ì •ìˆ˜ ìœ ì§€)\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        inputs[k] = v.to(device=device, dtype=dtype) if v.dtype.is_floating_point else v.to(device=device)\n",
    "\n",
    "# pad_token_id ì•ˆì „ìž¥ì¹˜\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=120, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "print(f\"ì†Œìš”ì‹œê°„ {time.time() - start:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2741774a-1b05-4b02-bbc6-5f48d99ac3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are a helpful assistant.\n",
      "\n",
      "ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ìž…ë ¥ë°›ì•„ì„œ ìˆ˜í–‰í•  ìˆ˜ ìžˆì–´?\n",
      "model\n",
      "ë„¤, ì €ëŠ” ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ìž…ë ¥ë°›ì•„ ì²˜ë¦¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "ì œê°€ í•  ìˆ˜ ìžˆëŠ” ìž‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "* **ì´ë¯¸ì§€ ë¶„ì„:** ì´ë¯¸ì§€ ë‚´ì˜ ê°ì²´ ì¸ì‹, ì´ë¯¸ì§€ ìº¡ì…”ë‹, ì´ë¯¸ì§€ ë¶„ë¥˜ ë“± ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ë¶„ì„ ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
      "* **í…ìŠ¤íŠ¸ ë¶„ì„:** í…ìŠ¤íŠ¸ ë²ˆì—­, ìš”ì•½, ì§ˆë¬¸ ë‹µë³€, ê°ì„± ë¶„ì„, í…ìŠ¤íŠ¸ ìƒì„± ë“± ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ë¶„ì„ ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
      "* **ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì—°ë™:** ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì´í•´í•˜ê³ , ì´ë¯¸ì§€ì— ëŒ€í•œ ì„¤ëª…ì„ ìƒì„±í•˜ê±°ë‚˜, í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì´ë¯¸ì§€ ê²€ìƒ‰ ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**ì–´ë–»ê²Œ ì‚¬ìš©í•˜ì‹œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?**\n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì„ í•˜ì‹¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤:\n",
      "\n",
      "* \"ì´ ì‚¬ì§„ì— ìžˆëŠ” ì‚¬ëžŒë“¤ì˜ ê°ì •ì„ ë¶„ì„í•´ì¤˜.\" (ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì‚¬ì§„ì„ ì—…ë¡œë“œ)\n",
      "* \"ì´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì¤˜.\" (í…ìŠ¤íŠ¸ë¥¼ ìž…ë ¥)\n",
      "* \"ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜.\" (ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œ)\n",
      "* \"ì´ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ë‹¨ì–´ë¥¼ ì°¾ì•„ì¤˜.\" (í…ìŠ¤íŠ¸ë¥¼ ìž…ë ¥)\n",
      "\n",
      "ì–´ë–¤ ìž‘ì—…ì„\n",
      "ì†Œìš”ì‹œê°„ 79.304s\n"
     ]
    }
   ],
   "source": [
    "import time, copy\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "message_template = [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": \"{input}\"}]},\n",
    "]\n",
    "\n",
    "user_input = 'ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ìž…ë ¥ë°›ì•„ì„œ ìˆ˜í–‰í•  ìˆ˜ ìžˆì–´?'  # ì˜ˆì‹œ\n",
    "\n",
    "# âœ… í…œí”Œë¦¿ì„ ë³µì‚¬í•´ì„œ ì•ˆì „í•˜ê²Œ ì±„ìš°ê¸°\n",
    "messages = copy.deepcopy(message_template)\n",
    "messages[1][\"content\"][0][\"text\"] = user_input\n",
    "\n",
    "# âœ… ë¦¬ìŠ¤íŠ¸ í˜•íƒœ(conversation)ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,   # ë‹¤ìŒ í„´ì´ assistantê°€ ë˜ë„ë¡\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤/ì •ë°€ë„ ì´ë™ (ì •ìˆ˜ëŠ” ì •ìˆ˜ ìœ ì§€)\n",
    "device = next(model.parameters()).device\n",
    "for k, v in list(inputs.items()):\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        inputs[k] = v.to(device=device, dtype=dtype) if v.dtype.is_floating_point else v.to(device=device)\n",
    "\n",
    "# pad_token_id ì•ˆì „ìž¥ì¹˜\n",
    "if model.generation_config.pad_token_id is None and processor.tokenizer.eos_token_id is not None:\n",
    "    model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
    "\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "print(f\"ì†Œìš”ì‹œê°„ {time.time() - start:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a296d49c-835f-4456-b584-1405f8032ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
